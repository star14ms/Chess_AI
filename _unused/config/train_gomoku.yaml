# @package _global_
# config/train_gomoku.yaml

defaults:
  - _self_ # Allows values defined here to be defaults

hydra:
  run:
    dir: outputs/gomoku_train/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/gomoku_train/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Target the main Config dataclass
_target_: utils.config_schema_rl.Config

network:
  # Target the specific NetworkConfig dataclass
  _target_: utils.config_schema_rl.NetworkConfig
  # Values here override defaults in the dataclass
  input_channels: 2  # Black and white pieces
  dim_piece_type: 0  # Only two piece types in Gomoku
  board_size: 15  # Standard Gomoku board size
  num_residual_layers: 0
  initial_conv_block_out_channels: [32, 32, 32, 32, 32, 32, 32]
  residual_blocks_out_channels: []
  action_space_size: 225  # 15x15 board
  num_pieces: 2  # Only black and white pieces
  policy_head_out_channels: 4
  value_head_hidden_size: 64

# Environment configuration
env:
  _target_: utils.config_schema_rl.EnvConfig
  type: gomoku  # Specify Gomoku environment
  observation_mode: vector
  render_mode: rgb_array
  save_video_folder: './videos'

mcts:
  _target_: utils.config_schema_rl.MCTSConfig
  iterations: 0
  c_puct: 1.41
  temperature_start: 1.0
  temperature_end: 0.1
  temperature_decay_moves: 30
  batch_size: 8  # Number of leaves to evaluate in one batched network call

optimizer:
  _target_: utils.config_schema_rl.OptimizerConfig
  type: Adam
  learning_rate: 0.1
  weight_decay: 1e-4

training:
  _target_: utils.config_schema_rl.TrainingConfig
  replay_buffer_size: 8192
  device: auto
  num_training_iterations: 1000
  num_training_steps: 50
  batch_size: 64
  use_multiprocessing: false
  self_play_workers: null
  self_play_games_per_epoch: 24
  max_game_moves: 225  # Maximum possible moves in a 15x15 board
  checkpoint_dir: ./checkpoints
  checkpoint_load: null  # If set, load checkpoints from here; otherwise use checkpoint_dir
  initial_board_fen: null  # Not used for Gomoku 
  progress_bar: true