{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import gymnasium as gym\n",
    "import chess_gym\n",
    "\n",
    "env = gym.make(\"Chess-v0\")\n",
    "env.reset()\n",
    "board = env.action_space.board\n",
    "\n",
    "display(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from MCTS.models.network import ChessNetwork\n",
    "from chess_gym.chess_custom import FullyTrackedBoard\n",
    "\n",
    "# Load config\n",
    "# Load config directly without hydra decorator\n",
    "checkpoint_path = \"../checkpoints/model.pth\"\n",
    "cfg = OmegaConf.load(\"../config/train_mcts.yaml\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Initialize network with config parameters\n",
    "model = ChessNetwork(\n",
    "    input_channels=cfg.network.input_channels,\n",
    "    dim_piece_type=cfg.network.dim_piece_type,\n",
    "    board_size=cfg.network.board_size,\n",
    "    num_residual_layers=cfg.network.num_residual_layers,\n",
    "    num_filters=cfg.network.num_filters,\n",
    "    conv_blocks_channel_lists=cfg.network.conv_blocks_channel_lists,\n",
    "    action_space_size=cfg.network.action_space_size,\n",
    "    num_pieces=cfg.network.num_pieces,\n",
    "    value_head_hidden_size=cfg.network.value_head_hidden_size\n",
    ")\n",
    "# Load the model from checkpoint\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu')['model_state_dict'])\n",
    "print('Model loaded!')\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "\n",
    "def execute_policy(model: nn.Module, board: FullyTrackedBoard, device=device, deterministic=True):\n",
    "    \"\"\"\n",
    "    Execute policy by selecting the highest probability action from model's output distribution.\n",
    "    \n",
    "    Args:\n",
    "        model: ChessNetwork model instance\n",
    "        board: Chess board instance\n",
    "        device: Device to run model on ('cpu' or 'cuda')\n",
    "        \n",
    "    Returns:\n",
    "        action: Selected chess move\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    observation = torch.from_numpy(board.get_board_vector()).unsqueeze(0).to(device=device, dtype=torch.float32)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        policy, _ = model(observation)\n",
    "        policy = policy.squeeze()  # Remove batch dimension\n",
    "        \n",
    "    # Get highest probability legal action\n",
    "    # Convert policy logits to probabilities using softmax\n",
    "    policy_probs = torch.softmax(policy, dim=0)\n",
    "\n",
    "    if deterministic:\n",
    "        # Sample action deterministically based on highest probability\n",
    "        action_id = policy.argmax().item() + 1\n",
    "    else:\n",
    "        # Sample action stochastically based on probabilities\n",
    "        action_id = torch.multinomial(policy_probs, num_samples=1).item() + 1\n",
    "\n",
    "    return action_id, policy_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_game_result(board, reward, terminated, truncated):\n",
    "  # --- After the loop ---\n",
    "  # Print game result\n",
    "  if terminated or truncated: # Check why the loop ended\n",
    "      if board.is_foul():\n",
    "        print(\"Game ended in a foul!\")\n",
    "      if reward == 1:\n",
    "        print(\"White wins!\")\n",
    "      elif reward == -1:\n",
    "        print(\"Black wins!\")\n",
    "      elif board.is_stalemate():\n",
    "          print(\"It's a stalemate!\")\n",
    "      elif board.is_insufficient_material():\n",
    "          print(\"It's a draw due to insufficient material!\")\n",
    "      elif board.can_claim_draw():\n",
    "          print(\"It's a draw by repetition or 50-move rule!\")\n",
    "      elif truncated:\n",
    "          print(\"Game truncated.\")\n",
    "      else:\n",
    "          print(\"Game ended in a draw (other reason).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation with Human-Made Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from utils.policy_human import sample_action\n",
    "from utils.visualize import board_to_svg, display_svgs_horizontally, visualize_policy_on_board, draw_possible_actions_on_board\n",
    "from IPython.display import SVG\n",
    "from utils.analyze import interpret_action\n",
    "\n",
    "env = gym.make(\"Chess-v0\", render_mode='rgb_array', show_possible_actions=False)\n",
    "env.reset()\n",
    "board = env.action_space.board\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "last_svg_str = None # To store the SVG from the previous step\n",
    "step_count = 0\n",
    "board_size = 500\n",
    "deterministic = False\n",
    "\n",
    "# Display initial board state\n",
    "initial_svg = board_to_svg(board, size=board_size)\n",
    "display(SVG(initial_svg))\n",
    "last_svg_str = None # Store the first SVG\n",
    "last_log_str = '' # Store the first log\n",
    "last_policy_probs = None\n",
    "\n",
    "print(\"Starting Game...\")\n",
    "\n",
    "while not terminated and not truncated:\n",
    "  action, policy_probs = execute_policy(model, board, deterministic=deterministic) # Get action and policy info\n",
    "\n",
    "  if action not in board.legal_actions:\n",
    "    action_info = interpret_action(action)\n",
    "    if action_info:\n",
    "        last_log_str += f\"Step {step_count + 1}: Action {action} (Color: {action_info['piece_color']}, Piece: {action_info['piece_type_str']}, Move: {action_info['move_type']})\\n\"\n",
    "    else:\n",
    "        last_log_str += f\"Step {step_count + 1}: Action {action} (Invalid/Unknown Action)\\n\"\n",
    "  else:\n",
    "    last_log_str += f\"Step {step_count + 1}: Action {action} ({board.san(action)})\\n\"\n",
    "\n",
    "  current_policy_probs = visualize_policy_on_board(board, policy_probs, board_size=board_size)\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  step_count += 1\n",
    "  current_svg_str = board_to_svg(board, size=board_size)\n",
    "\n",
    "  # display(draw_possible_actions_on_board(board, draw_action_ids=True))\n",
    "  # current_policy_distribution = visualize_policy_distribution(policy_probs.cpu().numpy(), 0, board)\n",
    "\n",
    "  # Display pairs of boards\n",
    "  if last_svg_str: # If we have a stored SVG from the previous step\n",
    "      print(\"Displaying boards from previous and current step:\")\n",
    "      display_svgs_horizontally([last_policy_probs, last_svg_str, current_policy_probs, current_svg_str])\n",
    "      print(last_log_str)\n",
    "      last_svg_str = None # Clear the stored SVG\n",
    "      last_log_str = ''\n",
    "      last_policy_probs = None\n",
    "  else: # If it's an odd step number (1st, 3rd, etc.), store the current SVG\n",
    "      last_svg_str = current_svg_str\n",
    "      last_policy_probs = current_policy_probs\n",
    "else:\n",
    "  if last_svg_str:\n",
    "    print(\"Displaying final board state:\")\n",
    "    display_svgs_horizontally([last_policy_probs, last_svg_str])\n",
    "    print(last_log_str)\n",
    "\n",
    "print_game_result(board, reward, terminated, truncated)\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
