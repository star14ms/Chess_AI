# @package _global_
# config/train_mcts.yaml

defaults:
  - _self_ # Allows values defined here to be defaults
#  - override hydra/job_logging: colorlog # Optional: for nicer logging
#  - override hydra/hydra_logging: colorlog # Optional: for nicer logging

hydra:
  run:
    dir: outputs/mcts_train/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/mcts_train/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Target the main Config dataclass
_target_: MCTS.config_schema.Config

network:
  # Target the specific NetworkConfig dataclass
  _target_: MCTS.config_schema.NetworkConfig
  # Values here override defaults in the dataclass
  input_channels: 11
  board_size: 8
  num_conv_layers: 1
  conv_blocks_channel_lists: [[16, 16, 16, 16, 16, 16, 16]]
  num_attention_heads: 8
  decoder_ff_dim_mult: 4
  action_space_size: 1700
  num_pieces: 32
  policy_hidden_size: 128

# New Env section
env:
  _target_: MCTS.config_schema.EnvConfig
  observation_mode: vector
  render_mode: rgb_array # Set to null or remove for no rendering
  save_video_folder: './videos' # Set to null or remove for no video saving

mcts:
  _target_: MCTS.config_schema.MCTSConfig
  iterations: 1
  c_puct: 1.41
  temperature_start: 1.0
  temperature_end: 0.1
  temperature_decay_moves: 30

optimizer:
  _target_: MCTS.config_schema.OptimizerConfig
  type: Adam
  learning_rate: 0.001
  # momentum: 0.9 # Not needed for Adam default
  weight_decay: 1e-4

training:
  _target_: MCTS.config_schema.TrainingConfig
  device: auto
  replay_buffer_size: 50000
  num_training_iterations: 100
  training_epochs: 10
  batch_size: 16
  use_multiprocessing: true # Set to false for sequential self-play
  self_play_workers: 8 # Example: Use 8 parallel workers for self-play games
  self_play_games_per_epoch: 32
  max_game_moves: 1
  checkpoint_dir: ${hydra:run.dir}/checkpoints # Relative to hydra output
  save_interval: 10
  board_cls_str: chess_gym.chess_custom.FullyTrackedBoard
