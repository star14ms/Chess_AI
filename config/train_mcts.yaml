# @package _global_
# config/train_mcts.yaml

defaults:
  - _self_ # Allows values defined here to be defaults
#  - override hydra/job_logging: colorlog # Optional: for nicer logging
#  - override hydra/hydra_logging: colorlog # Optional: for nicer logging

hydra:
  run:
    dir: outputs/mcts_train/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/mcts_train/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Target the main Config dataclass
_target_: MCTS.config_schema.Config

network:
  # Target the specific NetworkConfig dataclass
  _target_: MCTS.config_schema.NetworkConfig
  # Values here override defaults in the dataclass
  input_channels: 14
  board_size: 8
  num_residual_layers: 7
  conv_blocks_channel_lists: [[32, 32], [32, 32], [32, 32], [32, 32], [32, 32], [32, 32], [32, 32]]
  action_space_size: 1700
  num_pieces: 32
  value_head_hidden_size: 256

# New Env section
env:
  _target_: MCTS.config_schema.EnvConfig
  observation_mode: vector
  render_mode: rgb_array # Set to null or remove for no rendering
  save_video_folder: './videos' # Set to null or remove for no video saving

mcts:
  _target_: MCTS.config_schema.MCTSConfig
  iterations: 32
  c_puct: 1.41
  temperature_start: 1.0
  temperature_end: 0.1
  temperature_decay_moves: 30

optimizer:
  _target_: MCTS.config_schema.OptimizerConfig
  type: Adam
  learning_rate: 0.1
  # momentum: 0.9 # Not needed for Adam default
  weight_decay: 1e-4

training:
  _target_: MCTS.config_schema.TrainingConfig
  device: auto
  replay_buffer_size: 10000
  num_training_iterations: 10000
  training_epochs: 30
  batch_size: 64
  use_multiprocessing: true # Set to false for sequential self-play
  self_play_workers: 8 # Example: Use 8 parallel workers for self-play games
  self_play_games_per_epoch: 1000
  max_game_moves: 200
  checkpoint_dir: ${hydra:run.dir}/checkpoints # Relative to hydra output
  save_interval: 10
  board_cls_str: chess_gym.chess_custom.FullyTrackedBoard
